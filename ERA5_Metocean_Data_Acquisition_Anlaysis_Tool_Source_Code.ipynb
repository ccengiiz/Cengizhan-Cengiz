{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import calendar\n",
    "import math\n",
    "import os\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Third-party library imports for data manipulation and analysis\n",
    "import cdsapi\n",
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "# Third-party library imports for scientific computing and statistics\n",
    "from scipy import stats\n",
    "from scipy.optimize import curve_fit\n",
    "import scipy.integrate as spi\n",
    "\n",
    "# Third-party library imports for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "\n",
    "# TODO: Set the directory where the data files will be saved with your specific path.\n",
    "nc_files_directory = '' # e.g. 'C:\\\\Users\\\\era5_.nc_files'\n",
    "\n",
    "# TODO: Set the directory where the data acquisition information will be saved with your specific path.\n",
    "data_acquisition_info_directory = '' # e.g. 'C:\\\\Users\\\\da_info'\n",
    "\n",
    "# TODO: Customize the filename of the output Excel file to contain the combined ERA5 data.\n",
    "output_excel_file = 'combined_era5_data.xlsx'\n",
    "output_excel_file_path = os.path.join(data_acquisition_info_directory, output_excel_file)\n",
    "\n",
    "# TODO: Customize the filename of the analysis to an Excel file\n",
    "output_analysis_excel_file = 'era5_monthly_weather_conditions.xlsx' #e.g. 'C:\\\\Users\\\\era5_files\\\\era5_monthly_weather_conditions.xlsx'\n",
    "output_analysis_excel_file_path = os.path.join(data_acquisition_info_directory, output_analysis_excel_file)\n",
    "\n",
    "# TODO: Customize the filename of the DataFrame to an Excel file for use in simulation tools\n",
    "output_wc_excel_file = 'era5_metocean_catgories_in_8hr_intervals.xlsx'  # e.g. 'C:\\\\Users\\\\era5_files\\\\era5_metocean_catgories_in_8hr_intervals.xlsx' \n",
    "output_wc_excel_file_path = os.path.join(data_acquisition_info_directory, output_wc_excel_file)\n",
    "\n",
    "# TODO: Set range of years to download data\n",
    "start_year = 1972\n",
    "end_year = 2020\n",
    "\n",
    "# Create the nc_files_directory if it does not exist\n",
    "if not os.path.exists(nc_files_directory):\n",
    "    os.makedirs(nc_files_directory)\n",
    "\n",
    "# Create the data_acquisition_info_directory if it does not exist\n",
    "if not os.path.exists(data_acquisition_info_directory):\n",
    "    os.makedirs(data_acquisition_info_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Metocean Data Acquisition from ERA5 Dataset\n",
    "\n",
    "# Initialize the Climate Data Store API client -- Make sure to have an account with the Copernicus Climate Data Store at https://cds.climate.copernicus.eu, which is necessary to access ERA5 data.\n",
    "c = cdsapi.Client()\n",
    "\n",
    "# Looping through the years to retrieve data\n",
    "for year in range(start_year, end_year): # TODO Adjust the loop range according to year of analysis interest \n",
    "    print(year)\n",
    "    # Constructing the filename for each year's data\n",
    "    output_filename = os.path.join(nc_files_directory, f'era5_data_{year}.nc') # could be named as; f'era5_data_humboltbayport_{year}.nc') -- depending on the selected location for data collection, port or wea.\n",
    "\n",
    "    # Retrieving data from the ERA5 dataset -- modify this section with desired parameter data and range collection\n",
    "    c.retrieve(\n",
    "        'reanalysis-era5-single-levels',\n",
    "        {\n",
    "            'product_type': 'reanalysis',\n",
    "            'variable': [\n",
    "                '10m_u_component_of_wind', '10m_v_component_of_wind',\n",
    "                'significant_height_of_wind_waves',\n",
    "            ],\n",
    "            'year': str(year),\n",
    "            'month': ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12'],\n",
    "            'day': ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31'],\n",
    "            'time': ['00:00', '01:00', '02:00', '03:00', '04:00', '05:00', '06:00', '07:00', '08:00', '09:00', '10:00', '11:00', '12:00', '13:00', '14:00', '15:00', '16:00', '17:00', '18:00', '19:00', '20:00', '21:00', '22:00', '23:00'],\n",
    "            'area': [],  # TODO Specify the geographical area (lat/lon), such as [35.25, -120.55, 35.15, -120.5,]\n",
    "            'format': 'netcdf',\n",
    "        },\n",
    "        output_filename\n",
    "    )\n",
    "\n",
    "# Confirmation message after downloading the data\n",
    "print(f'Data downloaded to {nc_files_directory}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Processing and Combining ERA5 Metocean .nc File Data into Excel File\n",
    "\n",
    "# Create an empty DataFrame to store the combined data\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "# Loop through the NetCDF files for collected data year ranges, e.g., 1972 to 2023\n",
    "for year in range(start_year, end_year):\n",
    "    # Construct the file path for each year's data\n",
    "    # TODO: Customize the filename based on your naming convention and location\n",
    "    file_path = os.path.join(nc_files_directory, f\"era5_data_{year}.nc\")\n",
    "\n",
    "    # Check if the file exists\n",
    "    if not os.path.exists(file_path):\n",
    "        continue\n",
    "\n",
    "    # Open the NetCDF file\n",
    "    data = nc.Dataset(file_path, 'r')\n",
    "\n",
    "    # Extract and process the data from the NetCDF file\n",
    "    # TODO: Adjust variable names as needed based on your NetCDF file structure. (the first element for the second and third dimensions of u10, v10, and shww selected)\n",
    "    times = data.variables['time'][:]\n",
    "    u10 = data.variables['u10'][:, 0, 0]\n",
    "    v10 = data.variables['v10'][:, 0, 0]\n",
    "    shww = data.variables['shww'][:, 0, 0]\n",
    "\n",
    "    # Calculate wind speed at 150m from the 10m level using the shear power law\n",
    "    wind_speed_10m = (u10**2 + v10**2)**0.5\n",
    "    wind_speed_150m = wind_speed_10m * (150/10)**0.14\n",
    "\n",
    "    # Flatten the arrays for processing\n",
    "    times = times.flatten()\n",
    "    wind_speed_10m = wind_speed_10m.flatten()\n",
    "    wind_speed_150m = wind_speed_150m.flatten()\n",
    "    shww = shww.flatten()\n",
    "\n",
    "    # Convert the time data to datetime format\n",
    "    # Adjust ref_time based on ERA5 reference time (e.g., 1900 or 1970)\n",
    "    ref_time = datetime(1900, 1, 1)  \n",
    "    times = [ref_time + timedelta(hours=int(t)) for t in times]\n",
    "\n",
    "    # Create a DataFrame for the current year's data\n",
    "    year_df = pd.DataFrame({\n",
    "        'Time (h)': times,\n",
    "        'Wind Speed at 10m (m/s)': wind_speed_10m,\n",
    "        'Wind Speed at 150m (m/s)': wind_speed_150m,\n",
    "        'Significant Wave Height (m)': shww\n",
    "    })\n",
    "\n",
    "    # Append the year's data to the combined DataFrame\n",
    "    combined_df = pd.concat([combined_df, year_df], ignore_index=True)\n",
    "\n",
    "# Save the combined data to an Excel file\n",
    "combined_df.to_excel(output_excel_file_path, index=False)\n",
    "print(f'Data from {start_year} to {end_year} exported to {output_excel_file}')\n",
    "\n",
    "# Optional: Print variable keys of the last opened NetCDF file for verification\n",
    "data = nc.Dataset(file_path, \"r\")\n",
    "print(data.variables.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Load Data\n",
    "\n",
    "df = pd.read_excel(output_excel_file_path)\n",
    "\n",
    "# Display the first few rows of the data\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Statistical Analysis of Wind Speed Data Using Weibull Distribution\n",
    "\n",
    "# Fit the Weibull distribution to wind speed data at 150m\n",
    "# TODO: Ensure your DataFrame 'df' has a column 'Wind Speed at 150m (m/s)'\n",
    "shape, loc, scale = stats.weibull_min.fit(df['Wind Speed at 150m (m/s)'], floc=0)\n",
    "\n",
    "# Generate a range of wind speeds for plotting the PDF and CDF\n",
    "x = np.linspace(0, df['Wind Speed at 150m (m/s)'].max(), 100)\n",
    "\n",
    "# Setting custom font for the plots\n",
    "plt.rcParams['font.family'] = 'Calibri'\n",
    "\n",
    "# Calculate the Probability Density Function (PDF) and Cumulative Distribution Function (CDF)\n",
    "pdf = stats.weibull_min.pdf(x, shape, loc, scale)\n",
    "cdf = stats.weibull_min.cdf(x, shape, loc, scale)\n",
    "\n",
    "# Create subplots for PDF and CDF\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "fig.set_size_inches(18, 6)  # Adjusting the figure size\n",
    "\n",
    "# Plotting the PDF\n",
    "ax1.plot(x, pdf, label='PDF (Weibull)')\n",
    "ax1.set_xlabel('Wind Speed at 150m (m/s)', fontsize=15)\n",
    "ax1.set_ylabel('Probability Density', fontsize=15)\n",
    "ax1.set_title('Wind Speed Probability Density Function (PDF)', fontsize=15)\n",
    "ax1.legend()\n",
    "ax1.tick_params(axis='both', labelsize=14)\n",
    "\n",
    "# Plotting the CDF\n",
    "ax2.plot(x, cdf, label='CDF (Weibull)')\n",
    "ax2.set_xlabel('Wind Speed at 150m (m/s)', fontsize=15)\n",
    "ax2.set_ylabel('Cumulative Probability', fontsize=15)\n",
    "ax2.set_title('Wind Speed Cumulative Distribution Function (CDF)', fontsize=15)\n",
    "ax2.legend()\n",
    "ax2.tick_params(axis='both', labelsize=14)\n",
    "\n",
    "# Display the plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Statistical Analysis of Wave Height Data Using Rayleigh Distribution\n",
    "\n",
    "# Fit a Rayleigh distribution to wave height data\n",
    "# TODO: Ensure your DataFrame 'df' has a column 'Significant Wave Height (m)'\n",
    "params = stats.rayleigh.fit(df['Significant Wave Height (m)'])\n",
    "scale = params[0] \n",
    "\n",
    "# Setting custom font for the plots\n",
    "plt.rcParams['font.family'] = 'Calibri'\n",
    "\n",
    "# Generate a range of wave heights for plotting the PDF and CDF\n",
    "x = np.linspace(0, df['Significant Wave Height (m)'].max(), 100)\n",
    "\n",
    "# Calculate the Probability Density Function (PDF) and Cumulative Distribution Function (CDF)\n",
    "pdf = stats.rayleigh.pdf(x, *params)\n",
    "cdf = stats.rayleigh.cdf(x, *params)\n",
    "\n",
    "# Create subplots for PDF and CDF\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "fig.set_size_inches(18, 6)  # Adjusting the figure size\n",
    "\n",
    "# Plotting the PDF\n",
    "ax1.plot(x, pdf, label='PDF (Rayleigh)')\n",
    "ax1.set_xlabel('Wave Height (m)', fontsize=15)\n",
    "ax1.set_ylabel('Probability Density', fontsize=15)\n",
    "ax1.set_title('Wave Height Probability Density Function (PDF)', fontsize=15)\n",
    "ax1.legend()\n",
    "ax1.tick_params(axis='both', labelsize=14)\n",
    "\n",
    "# Plotting the CDF\n",
    "ax2.plot(x, cdf, label='CDF (Rayleigh)')\n",
    "ax2.set_xlabel('Wave Height (m)', fontsize=15)\n",
    "ax2.set_ylabel('Cumulative Probability', fontsize=15)\n",
    "ax2.set_title('Wave Height Cumulative Distribution Function (CDF)', fontsize=15)\n",
    "ax2.legend()\n",
    "ax2.tick_params(axis='both', labelsize=14)\n",
    "\n",
    "# Display the plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Detailed Monthly Analysis of Weather Conditions in 8-Hour Intervals\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import calendar\n",
    "\n",
    "# Determine the range of years from the DataFrame\n",
    "unique_years = df['Time (h)'].dt.year.unique()\n",
    "years = range(min(unique_years), max(unique_years) + 1)\n",
    "months = list(calendar.month_name)[1:]  # List of month names\n",
    "\n",
    "# Initialize a dictionary to count conditions for each year and month\n",
    "conditions_count = {(year, month): [0, 0, 0, 0, 0] for year in years for month in months}\n",
    "\n",
    "def verify_condition(chunk, condition_met):\n",
    "    \"\"\"\n",
    "    Additional verification for certain weather conditions. \n",
    "    If initially determined as WC4 or WC5, check against WC3 or WC4 respectively.\n",
    "    \"\"\"\n",
    "    if condition_met == 3:  # If initially determined as WC4, check against WC3\n",
    "        without_extreme = chunk.drop(chunk['Wind Speed at 150m (m/s)'].idxmax())\n",
    "        without_extreme = without_extreme.drop(without_extreme['Significant Wave Height (m)'].idxmax())\n",
    "        if conditions[2](without_extreme).all():\n",
    "            return 2\n",
    "    elif condition_met == 4:  # If initially determined as WC5, check against WC4\n",
    "        without_extreme = chunk.drop(chunk['Wind Speed at 150m (m/s)'].idxmax())\n",
    "        without_extreme = without_extreme.drop(without_extreme['Significant Wave Height (m)'].idxmax())\n",
    "        if conditions[3](without_extreme).all():\n",
    "            return 3\n",
    "    return condition_met\n",
    "\n",
    "def determine_condition(chunk):\n",
    "    \"\"\"\n",
    "    Determine the weather condition for a given data chunk.\n",
    "    \"\"\"\n",
    "    for idx, condition in enumerate(conditions):\n",
    "        if condition(chunk).any():\n",
    "            return verify_condition(chunk, idx) if idx in [3, 4] else idx\n",
    "    return None\n",
    "\n",
    "# Defining weather conditions as functions for flexibility\n",
    "conditions = [\n",
    "    lambda chunk: (chunk['Wind Speed at 150m (m/s)'] < 6) | (chunk['Significant Wave Height (m)'] < 0.9),\n",
    "    lambda chunk: (chunk['Wind Speed at 150m (m/s)'] >= 6) & (chunk['Wind Speed at 150m (m/s)'] < 8) |\n",
    "                  (chunk['Significant Wave Height (m)'] >= 0.9) & (chunk['Significant Wave Height (m)'] < 1),\n",
    "    lambda chunk: (chunk['Wind Speed at 150m (m/s)'] >= 8) & (chunk['Wind Speed at 150m (m/s)'] < 12) |\n",
    "                  (chunk['Significant Wave Height (m)'] >= 1) & (chunk['Significant Wave Height (m)'] < 1.5),\n",
    "    lambda chunk: (chunk['Wind Speed at 150m (m/s)'] >= 12) & (chunk['Wind Speed at 150m (m/s)'] < 15) |\n",
    "                  (chunk['Significant Wave Height (m)'] >= 1.5) & (chunk['Significant Wave Height (m)'] < 3.5),\n",
    "    lambda chunk: (chunk['Wind Speed at 150m (m/s)'] >= 15) | (chunk['Significant Wave Height (m)'] >= 3.5)\n",
    "]\n",
    "\n",
    "# Data Pre-processing: Convert to numeric and handle non-numeric or negative values\n",
    "for column in ['Wind Speed at 150m (m/s)', 'Significant Wave Height (m)']:\n",
    "    df[column] = pd.to_numeric(df[column], errors='coerce').fillna(1).clip(lower=1)\n",
    "\n",
    "# Analyze each month of each year\n",
    "for year in years:\n",
    "    for month in months:\n",
    "        month_number = list(calendar.month_name).index(month)  # Convert month name to its number\n",
    "        monthly_data = df[(df['Time (h)'].dt.year == year) & (df['Time (h)'].dt.month == month_number)]\n",
    "        for i in range(0, len(monthly_data), 8):\n",
    "            chunk = monthly_data.iloc[i:i+8]\n",
    "            condition_met = determine_condition(chunk)\n",
    "            if condition_met is not None:\n",
    "                conditions_count[(year, month)][condition_met] += 1\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "output_df = pd.DataFrame.from_dict(conditions_count, orient='index', columns=['WC_1', 'WC_2', 'WC_3', 'WC_4', 'WC_5'])\n",
    "output_df.index = pd.MultiIndex.from_tuples(output_df.index, names=['Year', 'Month'])\n",
    "\n",
    "# Save the analysis to an Excel file\n",
    "output_df.to_excel(output_analysis_excel_file_path, engine='openpyxl')\n",
    "\n",
    "# Optionally, print a preview of the DataFrame\n",
    "print(output_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Visualization of Monthly Weather Condition Availabilities for Selected Location\n",
    "\n",
    "# Load the monthly weather condition data\n",
    "df = pd.read_excel(output_analysis_excel_file_path, engine='openpyxl')\n",
    "\n",
    "# Filling NaNs in the Year column to maintain continuity\n",
    "df['Year'].fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Specify the desired order of months for consistent plotting\n",
    "month_order = ['January', 'February', 'March', 'April', 'May', 'June', \n",
    "               'July', 'August', 'September', 'October', 'November', 'December']\n",
    "\n",
    "# Convert 'Year' and 'Month' to categorical data types for ordered plotting\n",
    "df['Year'] = df['Year'].astype(int).astype('category')\n",
    "df['Month'] = pd.Categorical(df['Month'], categories=month_order, ordered=True)\n",
    "\n",
    "# Prepare the data for 3D plotting\n",
    "x = df['Month'].cat.codes.values\n",
    "total_years = len(df['Year'].cat.categories)\n",
    "y_space = np.linspace(0, total_years-1, total_years)  # distant y values for each year\n",
    "y = np.repeat(y_space, len(df) // total_years)  # Repeat each year's data for each month\n",
    "\n",
    "# Normalize the heights to percentages for better comparison\n",
    "max_height = df[['WC_1', 'WC_2', 'WC_3', 'WC_4', 'WC_5']].sum(axis=1).max()\n",
    "df[['WC_1', 'WC_2', 'WC_3', 'WC_4', 'WC_5']] = (df[['WC_1', 'WC_2', 'WC_3', 'WC_4', 'WC_5']] / max_height) * 100\n",
    "\n",
    "# Generate labels for months and years\n",
    "month_labels = df['Month'].cat.categories.tolist()\n",
    "year_labels = [str(year) if year % 2 == 0 else '' for year in df['Year'].cat.categories.tolist()]\n",
    "\n",
    "# Initialize 3D plot\n",
    "fig = plt.figure(figsize=(24, 24))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.set_box_aspect([90,200,60])  # Adjust box aspect ratio\n",
    "\n",
    "# Define colors for different weather conditions\n",
    "colors = ['yellow', 'red', 'green', 'purple', 'pink']\n",
    "columns = ['WC_1', 'WC_2', 'WC_3', 'WC_4', 'WC_5']\n",
    "\n",
    "# Create stacked bar plots for each weather condition\n",
    "previous_height = np.zeros_like(df['WC_1'].values)\n",
    "for idx, col in enumerate(columns):\n",
    "    heights = df[col].values\n",
    "    ax.bar3d(x, y, previous_height, dx=1, dy=0.95, dz=heights, shade=True, color=colors[idx], alpha=0.4)\n",
    "    previous_height += heights\n",
    "\n",
    "# Add legend and labels for clarity\n",
    "legend_patches = [Patch(color=colors[i], label=columns[i]) for i in range(len(columns))]\n",
    "ax.legend(handles=legend_patches)\n",
    "ax.set_zlabel('Weather Condition Occurrence (%)', fontsize=14)\n",
    "ax.tick_params(axis='z', labelsize=14)\n",
    "ax.set_xticks(np.arange(len(month_labels)))\n",
    "ax.set_yticks(y_space)\n",
    "ax.set_xticklabels(month_labels, rotation=45, fontsize=14)\n",
    "ax.set_yticklabels(year_labels, fontsize=14)\n",
    "ax.view_init(elev=20, azim=-60)  # Adjust viewing angle for better visualization\n",
    "\n",
    "# Final layout adjustments and plot display\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) (IMPORTANT CELL IN METOCEAN DATA ANALYSIS FOR THE SIMULATION TOOL USE)\n",
    "# 8-hour Interval-Based Weather Condition Analysis for Offshore Wind Installation Operations\n",
    "\n",
    "# Print the DataFrame's column names to ensure required columns are present\n",
    "print(\"DataFrame columns:\", df.columns)\n",
    "\n",
    "# Determine the range of years from the DataFrame\n",
    "unique_years = df['Time (h)'].dt.year.unique()\n",
    "years = range(min(unique_years), max(unique_years) + 1)\n",
    "\n",
    "# Define weather conditions as lambda functions\n",
    "conditions = [\n",
    "    lambda chunk: (chunk['Wind Speed at 150m (m/s)'] < 6) | (chunk['Significant Wave Height (m)'] < 0.9),\n",
    "    lambda chunk: (chunk['Wind Speed at 150m (m/s)'] >= 6) & (chunk['Wind Speed at 150m (m/s)'] < 8) |\n",
    "                  (chunk['Significant Wave Height (m)'] >= 0.9) & (chunk['Significant Wave Height (m)'] < 1),\n",
    "    lambda chunk: (chunk['Wind Speed at 150m (m/s)'] >= 8) & (chunk['Wind Speed at 150m (m/s)'] < 12) |\n",
    "                  (chunk['Significant Wave Height (m)'] >= 1) & (chunk['Significant Wave Height (m)'] < 1.5),\n",
    "    lambda chunk: (chunk['Wind Speed at 150m (m/s)'] >= 12) & (chunk['Wind Speed at 150m (m/s)'] < 15) |\n",
    "                  (chunk['Significant Wave Height (m)'] >= 1.5) & (chunk['Significant Wave Height (m)'] < 3.5),\n",
    "    lambda chunk: (chunk['Wind Speed at 150m (m/s)'] >= 15) | (chunk['Significant Wave Height (m)'] >= 3.5)\n",
    "]\n",
    "\n",
    "# Function to verify and refine weather condition categorization\n",
    "def verify_condition(chunk, condition_met):\n",
    "    \"\"\"\n",
    "    Additional verification for certain weather conditions. \n",
    "    If initially determined as WC4 or WC5, check against WC3 or WC4 respectively.\n",
    "    \"\"\"\n",
    "    if condition_met == 3:  # If initially determined as WC4, check against WC3\n",
    "        without_extreme = chunk.drop(chunk['Wind Speed at 150m (m/s)'].idxmax())\n",
    "        without_extreme = without_extreme.drop(without_extreme['Significant Wave Height (m)'].idxmax())\n",
    "        if conditions[2](without_extreme).all():\n",
    "            return 2\n",
    "    elif condition_met == 4:  # If initially determined as WC5, check against WC4\n",
    "        without_extreme = chunk.drop(chunk['Wind Speed at 150m (m/s)'].idxmax())\n",
    "        without_extreme = without_extreme.drop(without_extreme['Significant Wave Height (m)'].idxmax())\n",
    "        if conditions[3](without_extreme).all():\n",
    "            return 3\n",
    "    return condition_met\n",
    "\n",
    "def determine_condition(chunk):\n",
    "    \"\"\"\n",
    "    Determine the weather condition for a given data chunk.\n",
    "    \"\"\"\n",
    "    for idx, condition in enumerate(conditions):\n",
    "        if condition(chunk).any():\n",
    "            # Additional verification for WC4 and WC5\n",
    "            if idx in [3, 4]:\n",
    "                return verify_condition(chunk, idx)\n",
    "            return idx\n",
    "    return None\n",
    "\n",
    "# Assuming 'df' is DataFrame containing the hourly weather data\n",
    "# DataFrame should have columns 'Time (h)', 'Wind Speed at 150m (m/s)', 'Significant Wave Height (m)'\n",
    "\n",
    "# Initialize a list to store data for interval-based weather condition analysis\n",
    "output_data = []\n",
    "\n",
    "# Iterate through the data in 8-hour intervals for each year\n",
    "for year in years:\n",
    "    yearly_data = df[df['Time (h)'].dt.year == year]\n",
    "    for i in range(0, len(yearly_data), 8):\n",
    "        chunk = yearly_data.iloc[i:i+8]\n",
    "        if len(chunk) == 8:  # Ensure each chunk represents a full 8-hour interval\n",
    "            start_time = chunk.iloc[0]['Time (h)']\n",
    "            end_time = chunk.iloc[-1]['Time (h)']\n",
    "            wc_category = 'WC_None'  # Default category if no condition is met\n",
    "            condition_met = determine_condition(chunk)\n",
    "            wc_category = f'WC_{condition_met + 1}' if condition_met is not None else wc_category\n",
    "            output_data.append({'Start Time': start_time, 'End Time': end_time, 'WC_Category': wc_category})\n",
    "\n",
    "# Convert the list to a DataFrame for better visualization and further analysis\n",
    "output_wc_df = pd.DataFrame(output_data)\n",
    "\n",
    "# Save the DataFrame to an Excel file for use in simulation tools\n",
    "output_wc_df.to_excel(output_wc_excel_file_path, index=False)\n",
    "\n",
    "# Optionally, print a preview of the DataFrame for verification\n",
    "print(output_wc_df.head())\n",
    "\n",
    "#Note: If 'KeyError: 'Time (h)' error occurs, rerun the Cell 3 and try again."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
